{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    exp = tf.exp(x - tf.reduce_max(x, reduction_indices = [1], keep_dims = True))\n",
    "    out = exp / tf.reduce_sum(exp, reduction_indices = [1])\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    out = -tf.reduce_sum((tf.log(yhat) * tf.to_float(y)))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142,  0.73105858],\n",
    "                                                      [0.26894142,  0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print (\"Basic (non-exhaustive) softmax tests pass\\n\")\n",
    "\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "            tf.constant(y, dtype=tf.int32),\n",
    "            tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print (\"Basic (non-exhaustive) cross-entropy tests pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "test_cross_entropy_loss_basic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
