{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import _pickle as pickle\n",
    "\n",
    "from model import Model\n",
    "from q2_initialization import xavier_weight_init\n",
    "from utils.general_utils import Progbar\n",
    "from utils.parser_utils import minibatches, load_and_preprocess_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_features = 36\n",
    "    n_classes = 3\n",
    "    dropout = 0.5\n",
    "    embed_size = 50\n",
    "    hidden_size = 200\n",
    "    batch_size = 2048\n",
    "    n_epochs = 10\n",
    "    lr = 0.001\n",
    "\n",
    "\n",
    "class ParserModel(Model):\n",
    "    \"\"\"\n",
    "    Implements a feedforward neural network with an embedding layer and single hidden layer.\n",
    "    This network will predict which transition should be applied to a given partial parse\n",
    "    configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building and will be fed\n",
    "        data during training.  Note that when \"None\" is in a placeholder's shape, it's flexible\n",
    "        (so we can use different batch sizes without rebuilding the model).\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of  shape (None, n_features), type tf.int32\n",
    "        labels_placeholder: Labels placeholder tensor of shape (None, n_classes), type tf.float32\n",
    "        dropout_placeholder: Dropout value placeholder (scalar), type tf.float32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "            self.dropout_placeholder\n",
    "\n",
    "        (Don't change the variable names)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.int32, [None, self.config.n_features])\n",
    "        self.labels_placeholder = tf.placeholder(tf.float32, [None, self.config.n_classes])\n",
    "        self.dropout_placeholder = tf.placeholder(tf.float32, None)     \n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=1):\n",
    "        \"\"\"Creates the feed_dict for the dependency parser.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "        Hint: When an argument is None, don't add it to the feed_dict.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "            dropout: The dropout rate.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.input_placeholder] = inputs_batch\n",
    "        if(labels_batch is not None): \n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "        feed_dict[self.dropout_placeholder] = dropout\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_embedding(self):\n",
    "        \"\"\"Adds an embedding layer that maps from input tokens (integers) to vectors and then\n",
    "        concatenates those vectors:\n",
    "            - Creates an embedding tensor and initializes it with self.pretrained_embeddings.\n",
    "            - Uses the input_placeholder to index into the embeddings tensor, resulting in a\n",
    "              tensor of shape (None, n_features, embedding_size).\n",
    "            - Concatenates the embeddings by reshaping the embeddings tensor to shape\n",
    "              (None, n_features * embedding_size).\n",
    "\n",
    "        Hint: You might find tf.nn.embedding_lookup useful.\n",
    "        Hint: You can use tf.reshape to concatenate the vectors. See following link to understand\n",
    "            what -1 in a shape means.\n",
    "            https://www.tensorflow.org/api_docs/python/array_ops/shapes_and_shaping#reshape.\n",
    "\n",
    "        Returns:\n",
    "            embeddings: tf.Tensor of shape (None, n_features*embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        embeddings = tf.nn.embedding_lookup(self.pretrained_embeddings, self.input_placeholder)\n",
    "        embeddings = tf.reshape(embeddings, [-1, self.config.n_features * self.config.embed_size])\n",
    "        ### END YOUR CODE\n",
    "        return embeddings\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the 1-hidden-layer NN:\n",
    "            h = Relu(xW + b1)\n",
    "            h_drop = Dropout(h, dropout_rate)\n",
    "            pred = h_dropU + b2\n",
    "\n",
    "        Note that we are not applying a softmax to pred. The softmax will instead be done in\n",
    "        the add_loss_op function, which improves efficiency because we can use\n",
    "        tf.nn.softmax_cross_entropy_with_logits\n",
    "\n",
    "        Use the initializer from q2_initialization.py to initialize W and U (you can initialize b1\n",
    "        and b2 with zeros)\n",
    "\n",
    "        Hint: Here are the dimensions of the various variables you will need to create\n",
    "                    W:  (n_features*embed_size, hidden_size)\n",
    "                    b1: (hidden_size,)\n",
    "                    U:  (hidden_size, n_classes)\n",
    "                    b2: (n_classes)\n",
    "        Hint: Note that tf.nn.dropout takes the keep probability (1 - p_drop) as an argument. \n",
    "            The keep probability should be set to the value of self.dropout_placeholder\n",
    "\n",
    "        Returns:\n",
    "            pred: tf.Tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.add_embedding()\n",
    "        ### YOUR CODE HERE\n",
    "        weight_initializer = xavier_weight_init()\n",
    "        self.W = tf.Variable(weight_initializer((self.config.n_features*self.config.embed_size, self.config.hidden_size)))\n",
    "        self.b1 = tf.Variable(tf.zeros(self.config.hidden_size, tf.float32))\n",
    "        self.U = tf.Variable(weight_initializer((self.config.hidden_size, self.config.n_classes)))\n",
    "        self.b2 = tf.Variable(tf.zeros(self.config.n_classes, tf.float32))\n",
    "        \n",
    "        h = tf.nn.relu(tf.matmul(x, self.W) + self.b1)\n",
    "        h_drop = tf.nn.dropout(h, 1 - self.config.dropout)\n",
    "        pred = tf.matmul(h_drop, self.U) + self.b2\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "        In this case we are using cross entropy loss.\n",
    "        The loss should be averaged over all examples in the current minibatch.\n",
    "\n",
    "        Hint: You can use tf.nn.softmax_cross_entropy_with_logits to simplify your\n",
    "                    implementation. You might find tf.reduce_mean useful.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes) containing the output of the neural\n",
    "                  network before the softmax layer.\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.labels_placeholder, logits = pred))\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Use tf.train.AdamOptimizer for this model.\n",
    "        Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate = self.config.lr).minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch,\n",
    "                                     dropout=self.config.dropout)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def run_epoch(self, sess, parser, train_examples, dev_set):\n",
    "        prog = Progbar(target=1 + len(train_examples) / self.config.batch_size)\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)):\n",
    "            loss = self.train_on_batch(sess, train_x, train_y)\n",
    "            prog.update(i + 1, [(\"train loss\", loss)])\n",
    "\n",
    "        print (\"Evaluating on dev set\",)\n",
    "        dev_UAS, _ = parser.parse(dev_set)\n",
    "        print (\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "        return dev_UAS\n",
    "\n",
    "    def fit(self, sess, saver, parser, train_examples, dev_set):\n",
    "        best_dev_UAS = 0\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            print (\"Epoch {:} out of {:}\".format(epoch + 1, self.config.n_epochs))\n",
    "            dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set)\n",
    "            if dev_UAS > best_dev_UAS:\n",
    "                best_dev_UAS = dev_UAS\n",
    "                if saver:\n",
    "                    print( \"New best dev UAS! Saving model in ./data/weights/parser.weights\")\n",
    "                    saver.save(sess, './data/weights/parser.weights')\n",
    "            print()\n",
    "\n",
    "    def __init__(self, config, pretrained_embeddings):\n",
    "        self.pretrained_embeddings = pretrained_embeddings\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def main(debug=True):\n",
    "    print (80 * \"=\")\n",
    "    print (\"INITIALIZING\")\n",
    "    print( 80 * \"=\")\n",
    "    config = Config()\n",
    "    parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug)\n",
    "    if not os.path.exists('./data/weights/'):\n",
    "        os.makedirs('./data/weights/')\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        print (\"Building model...\",)\n",
    "        start = time.time()\n",
    "        model = ParserModel(config, embeddings)\n",
    "        parser.model = model\n",
    "        print (\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        # If you are using an old version of TensorFlow, you may have to use\n",
    "        # this initializer instead.\n",
    "        # init = tf.initialize_all_variables()\n",
    "        saver = None if debug else tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as session:\n",
    "            parser.session = session\n",
    "            session.run(init)\n",
    "\n",
    "            print( 80 * \"=\")\n",
    "            print( \"TRAINING\")\n",
    "            print( 80 * \"=\")\n",
    "            model.fit(session, saver, parser, train_examples, dev_set)\n",
    "\n",
    "            if not debug:\n",
    "                print (80 * \"=\")\n",
    "                print (\"TESTING\")\n",
    "                print( 80 * \"=\")\n",
    "                print (\"Restoring the best model weights found on the dev set\")\n",
    "                saver.restore(session, './data/weights/parser.weights')\n",
    "                print (\"Final evaluation on test set\",)\n",
    "                UAS, dependencies = parser.parse(test_set)\n",
    "                print (\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "                print (\"Writing predictions\")\n",
    "                with open('q2_test.predicted.pkl', 'wb') as f:\n",
    "                    pickle.dump(dependencies, f, -1)\n",
    "                print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 3.19 seconds\n",
      "Building parser...\n",
      "took 0.05 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.69 seconds\n",
      "Vectorizing data...\n",
      "took 0.08 seconds\n",
      "Preprocessing training data...\n",
      "1000/1000 [==============================] - 6s     \n",
      "Building model...\n",
      "took 0.29 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n",
      "24/24 [============================>.] - ETA: 0s - train loss: 0.9124Evaluating on dev set\n",
      "- dev UAS: 41.84\n",
      "\n",
      "Epoch 2 out of 10\n",
      "24/24 [============================>.] - ETA: 0s - train loss: 0.4377Evaluating on dev set\n",
      "- dev UAS: 49.40\n",
      "\n",
      "Epoch 3 out of 10\n",
      "24/24 [============================>.] - ETA: 0s - train loss: 0.3487Evaluating on dev set\n",
      "- dev UAS: 55.88\n",
      "\n",
      "Epoch 4 out of 10\n",
      "24/24 [============================>.] - ETA: 0s - train loss: 0.3017Evaluating on dev set\n",
      "- dev UAS: 58.37\n",
      "\n",
      "Epoch 5 out of 10\n",
      "21/24 [========================>.....] - ETA: 0s - train loss: 0.2706"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING\n",
      "================================================================================\n",
      "Loading data...\n",
      "took 2.50 seconds\n",
      "Building parser...\n",
      "took 1.56 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.64 seconds\n",
      "Vectorizing data...\n",
      "took 1.67 seconds\n",
      "Preprocessing training data...\n",
      "39832/39832 [==============================] - 68s    \n",
      "Building model...\n",
      "input_placeholder Tensor(\"Placeholder:0\", shape=(?, 36), dtype=int32)\n",
      "labels_placeholder Tensor(\"Placeholder_1:0\", shape=(?, 3), dtype=float32)\n",
      "dropout_placeholder Tensor(\"Placeholder_2:0\", dtype=float32)\n",
      "took 0.26 seconds\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.1806Evaluating on dev set\n",
      "- dev UAS: 83.26\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "Epoch 2 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.1005Evaluating on dev set\n",
      "- dev UAS: 85.63\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "Epoch 3 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0876Evaluating on dev set\n",
      "- dev UAS: 85.35\n",
      "\n",
      "Epoch 4 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0793Evaluating on dev set\n",
      "- dev UAS: 85.96\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "Epoch 5 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0738Evaluating on dev set\n",
      "- dev UAS: 86.88\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "Epoch 6 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0694Evaluating on dev set\n",
      "- dev UAS: 87.01\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "Epoch 7 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0654Evaluating on dev set\n",
      "- dev UAS: 86.81\n",
      "\n",
      "Epoch 8 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0620Evaluating on dev set\n",
      "- dev UAS: 86.11\n",
      "\n",
      "Epoch 9 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0594Evaluating on dev set\n",
      "- dev UAS: 87.30\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "Epoch 10 out of 10\n",
      "924/924 [============================>.] - ETA: 0s - train loss: 0.0562Evaluating on dev set\n",
      "- dev UAS: 87.37\n",
      "New best dev UAS! Saving model in ./data/weights/parser.weights\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "INFO:tensorflow:Restoring parameters from ./data/weights/parser.weights\n",
      "Final evaluation on test set\n",
      "- test UAS: 87.79\n",
      "Writing predictions\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "main(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
