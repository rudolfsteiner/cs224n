{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q1_softmax import cross_entropy_loss\n",
    "from model import Model\n",
    "from utils.general_utils import get_minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2748a3e4860a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSoftmaxModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    batch_size = 64\n",
    "    n_epochs = 50\n",
    "    lr = 1e-4\n",
    "\n",
    "\n",
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                                              (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                                              (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.input_placeholder = tf.placeholder(tf.float32, [Config.batch_size, Config.n_features])\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, [Config.batch_size, Config.n_classes])\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be the placeholder\n",
    "                tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        feed_dict = {self.input_placeholder: inputs_batch, self.labels_placeholder: labels_batch}\n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the transformation is a linear layer plus a\n",
    "        softmax transformation:\n",
    "\n",
    "        y = softmax(Wx + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                    and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "            input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        self.W = tf.Variable(tf.zeros([Config.n_features, Config.n_classes]))\n",
    "        self.b = tf.Variable(tf.zeros([Config.n_classes]))\n",
    "        \n",
    "        hidden = tf.matmul(self.input_placeholder, self.W) + self.b        \n",
    "        pred = softmax(hidden)\n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                    short function.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                    Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        train_op = tf.train.GradientDescentOptimizer(Config.lr).minimize(loss)\n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def run_epoch(self, sess, inputs, labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session() object\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        n_minibatches, total_loss = 0, 0\n",
    "        for input_batch, labels_batch in get_minibatches([inputs, labels], self.config.batch_size):\n",
    "            n_minibatches += 1\n",
    "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
    "        return total_loss / n_minibatches\n",
    "\n",
    "    def fit(self, sess, inputs, labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, inputs, labels)\n",
    "            duration = time.time() - start_time\n",
    "            print('Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, average_loss, duration))\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.build()\n",
    "\n",
    "\n",
    "def test_softmax_model():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    # Generate random data to train the model on\n",
    "    np.random.seed(1234)\n",
    "    inputs = np.random.rand(config.n_samples, config.n_features)\n",
    "    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n",
    "    labels[:, 0] = 1\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    # (not required but good practice)\n",
    "    with tf.Graph().as_default():\n",
    "        # Build the model and add the variable initializer Op\n",
    "        model = SoftmaxModel(config)\n",
    "        init = tf.global_variables_initializer()\n",
    "        # If you are using an old version of TensorFlow, you may have to use\n",
    "        # this initializer instead.\n",
    "        # init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a session for running Ops in the Graph\n",
    "        with tf.Session() as sess:\n",
    "            # Run the Op to initialize the variables.\n",
    "            sess.run(init)\n",
    "            # Fit the model\n",
    "            losses = model.fit(sess, inputs, labels)\n",
    "\n",
    "    # If Ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print(\"Basic (non-exhaustive) classifier tests pass\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 59.18 (0.016 sec)\n",
      "Epoch 1: loss = 20.32 (0.021 sec)\n",
      "Epoch 2: loss = 10.92 (0.012 sec)\n",
      "Epoch 3: loss = 7.30 (0.012 sec)\n",
      "Epoch 4: loss = 5.44 (0.011 sec)\n",
      "Epoch 5: loss = 4.32 (0.011 sec)\n",
      "Epoch 6: loss = 3.58 (0.008 sec)\n",
      "Epoch 7: loss = 3.05 (0.000 sec)\n",
      "Epoch 8: loss = 2.65 (0.016 sec)\n",
      "Epoch 9: loss = 2.35 (0.016 sec)\n",
      "Epoch 10: loss = 2.11 (0.016 sec)\n",
      "Epoch 11: loss = 1.91 (0.000 sec)\n",
      "Epoch 12: loss = 1.75 (0.016 sec)\n",
      "Epoch 13: loss = 1.61 (0.016 sec)\n",
      "Epoch 14: loss = 1.49 (0.000 sec)\n",
      "Epoch 15: loss = 1.39 (0.016 sec)\n",
      "Epoch 16: loss = 1.30 (0.016 sec)\n",
      "Epoch 17: loss = 1.22 (0.016 sec)\n",
      "Epoch 18: loss = 1.15 (0.016 sec)\n",
      "Epoch 19: loss = 1.09 (0.016 sec)\n",
      "Epoch 20: loss = 1.03 (0.013 sec)\n",
      "Epoch 21: loss = 0.98 (0.011 sec)\n",
      "Epoch 22: loss = 0.94 (0.012 sec)\n",
      "Epoch 23: loss = 0.89 (0.006 sec)\n",
      "Epoch 24: loss = 0.86 (0.016 sec)\n",
      "Epoch 25: loss = 0.82 (0.000 sec)\n",
      "Epoch 26: loss = 0.79 (0.016 sec)\n",
      "Epoch 27: loss = 0.76 (0.016 sec)\n",
      "Epoch 28: loss = 0.73 (0.000 sec)\n",
      "Epoch 29: loss = 0.71 (0.016 sec)\n",
      "Epoch 30: loss = 0.68 (0.016 sec)\n",
      "Epoch 31: loss = 0.66 (0.016 sec)\n",
      "Epoch 32: loss = 0.64 (0.000 sec)\n",
      "Epoch 33: loss = 0.62 (0.016 sec)\n",
      "Epoch 34: loss = 0.60 (0.016 sec)\n",
      "Epoch 35: loss = 0.58 (0.020 sec)\n",
      "Epoch 36: loss = 0.57 (0.017 sec)\n",
      "Epoch 37: loss = 0.55 (0.011 sec)\n",
      "Epoch 38: loss = 0.54 (0.011 sec)\n",
      "Epoch 39: loss = 0.52 (0.010 sec)\n",
      "Epoch 40: loss = 0.51 (0.004 sec)\n",
      "Epoch 41: loss = 0.50 (0.016 sec)\n",
      "Epoch 42: loss = 0.48 (0.000 sec)\n",
      "Epoch 43: loss = 0.47 (0.016 sec)\n",
      "Epoch 44: loss = 0.46 (0.016 sec)\n",
      "Epoch 45: loss = 0.45 (0.000 sec)\n",
      "Epoch 46: loss = 0.44 (0.016 sec)\n",
      "Epoch 47: loss = 0.43 (0.016 sec)\n",
      "Epoch 48: loss = 0.42 (0.000 sec)\n",
      "Epoch 49: loss = 0.41 (0.016 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n"
     ]
    }
   ],
   "source": [
    "test_softmax_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
