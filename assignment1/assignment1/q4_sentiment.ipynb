{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from utils.treebank import StanfordSentiment\n",
    "import utils.glove as glove\n",
    "\n",
    "from q3_sgd import load_saved_params, sgd\n",
    "\n",
    "# We will use sklearn here because it will run faster than implementing\n",
    "# ourselves. However, for other parts of this assignment you must implement\n",
    "# the functions yourself!\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--pretrained\", dest=\"pretrained\", action=\"store_true\",\n",
    "                       help=\"Use pretrained GloVe vectors.\")\n",
    "    group.add_argument(\"--yourvectors\", dest=\"yourvectors\", action=\"store_true\",\n",
    "                       help=\"Use your vectors from q3.\")\n",
    "    print(parser.parse_args)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSentenceFeatures(tokens, wordVectors, sentence):\n",
    "    \"\"\"\n",
    "    Obtain the sentence feature for sentiment analysis by averaging its\n",
    "    word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement computation for the sentence features given a sentence.\n",
    "\n",
    "    # Inputs:\n",
    "    # tokens -- a dictionary that maps words to their indices in\n",
    "    #           the word vector list\n",
    "    # wordVectors -- word vectors (each row) for all tokens\n",
    "    # sentence -- a list of words in the sentence of interest\n",
    "\n",
    "    # Output:\n",
    "    # - sentVector: feature vector for the sentence\n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #for word in sentence:\n",
    "        #sentVector += wordVectors[tokens[word]]\n",
    "        \n",
    "    sentVectors = wordVectors[[tokens[x] for x in sentence]]\n",
    "    sentVector = np.sum(sentVectors, axis = 0) / len(sentence)\n",
    "    #sentVector /= len(sentence)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert sentVector.shape == (wordVectors.shape[1],)\n",
    "    return sentVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    values = None   # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values = np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on parameter tuning on the dev set\n",
    "\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    m = [x['dev'] for x in results]\n",
    "    bestResult = results[np.argmax(m)]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return bestResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "\n",
    "def plotRegVsAccuracy(regValues, results, filename):\n",
    "    \"\"\" Make a plot of regularization vs accuracy \"\"\"\n",
    "    plt.plot(regValues, [x[\"train\"] for x in results])\n",
    "    plt.plot(regValues, [x[\"dev\"] for x in results])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"regularization\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(['train', 'dev'], loc='upper left')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputConfusionMatrix(features, labels, clf, filename):\n",
    "    \"\"\" Generate a confusion matrix \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    cm = confusion_matrix(labels, pred, labels=range(5))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.colorbar()\n",
    "    classes = [\"- -\", \"-\", \"neut\", \"+\", \"+ +\"]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "def outputPredictions(dataset, features, labels, clf, filename):\n",
    "    \"\"\" Write the predictions to file \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    with open(filename, \"w\") as f:\n",
    "        print (\"True\\tPredicted\\tText\", file = f)\n",
    "        for i in xrange(len(dataset)):\n",
    "            print (\"%d\\t%d\\t%s\" % (\n",
    "                labels[i], pred[i], \" \".join(dataset[i][0])), file = f)\n",
    "\n",
    "\n",
    "def main(): #args):\n",
    "    \"\"\" Train a model to do sentiment analyis\"\"\"\n",
    "\n",
    "    argspretrained = True\n",
    "    \n",
    "    argsyourvectors = False\n",
    "    # Load the dataset\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "    print(\"here?\")\n",
    "    if argsyourvectors:\n",
    "        _, wordVectors, _ = load_saved_params()\n",
    "        wordVectors = np.concatenate(\n",
    "            (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "            axis=1)\n",
    "    elif argspretrained:\n",
    "        wordVectors = glove.loadWordVectors(tokens)\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "\n",
    "    # Load the train set\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "    for i in range(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "    for i in range(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare test set features\n",
    "    testset = dataset.getTestSentences()\n",
    "    nTest = len(testset)\n",
    "    testFeatures = np.zeros((nTest, dimVectors))\n",
    "    testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "    for i in range(nTest):\n",
    "        words, testLabels[i] = testset[i]\n",
    "        testFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # We will save our results from each run\n",
    "    results = []\n",
    "    regValues = getRegularizationValues()\n",
    "    for reg in regValues:\n",
    "        print (\"Training for reg=%f\" % reg)\n",
    "        # Note: add a very small number to regularization to please the library\n",
    "        clf = LogisticRegression(C=1.0/(reg + 1e-12))\n",
    "        clf.fit(trainFeatures, trainLabels)\n",
    "\n",
    "        # Test on train set\n",
    "        pred = clf.predict(trainFeatures)\n",
    "        trainAccuracy = accuracy(trainLabels, pred)\n",
    "        print (\"Train accuracy (%%): %f\" % trainAccuracy)\n",
    "\n",
    "        # Test on dev set\n",
    "        pred = clf.predict(devFeatures)\n",
    "        devAccuracy = accuracy(devLabels, pred)\n",
    "        print (\"Dev accuracy (%%): %f\" % devAccuracy)\n",
    "\n",
    "        # Test on test set\n",
    "        # Note: always running on test is poor style. Typically, you should\n",
    "        # do this only after validation.\n",
    "        pred = clf.predict(testFeatures)\n",
    "        testAccuracy = accuracy(testLabels, pred)\n",
    "        print (\"Test accuracy (%%): %f\" % testAccuracy)\n",
    "\n",
    "        results.append({\n",
    "            \"reg\": reg,\n",
    "            \"clf\": clf,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy})\n",
    "\n",
    "    # Print the accuracies\n",
    "    print (\"\")\n",
    "    print (\"=== Recap ===\")\n",
    "    print (\"Reg\\t\\tTrain\\tDev\\tTest\")\n",
    "    for result in results:\n",
    "        print (\"%.2E\\t%.3f\\t%.3f\\t%.3f\" % (\n",
    "            result[\"reg\"],\n",
    "            result[\"train\"],\n",
    "            result[\"dev\"],\n",
    "            result[\"test\"]))\n",
    "    print (\"\")\n",
    "\n",
    "    bestResult = chooseBestModel(results)\n",
    "    print (\"Best regularization value: %0.2E\" % bestResult[\"reg\"])\n",
    "    print (\"Test accuracy (%%): %f\" % bestResult[\"test\"])\n",
    "\n",
    "    # do some error analysis\n",
    "    if args.pretrained:\n",
    "        plotRegVsAccuracy(regValues, results, \"q4_reg_v_acc.png\")\n",
    "        outputConfusionMatrix(devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                              \"q4_dev_conf.png\")\n",
    "        outputPredictions(devset, devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                          \"q4_dev_pred.txt\")\n",
    "        \n",
    "#if __name__ == \"__main__\":\n",
    "    #main(getArguments())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here?\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-0e5ad2ce35e0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m             axis=1)\n\u001b[1;32m     66\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0margspretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mwordVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadWordVectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mdimVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Lenovo\\Downloads\\cs224nd\\assignment1\\assignment1\\utils\\glove.py\u001b[0m in \u001b[0;36mloadWordVectors\u001b[0;34m(tokens, filepath, dimensions)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwordVectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mifs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mifs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 2273: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
